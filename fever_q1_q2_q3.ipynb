{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_lines\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n",
    "import string\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#109 wiki pages\n",
    "#5416537 documents in wiki pages \n",
    "#type(documents) == list\n",
    "#type(documents[0]) == dict\n",
    "def load_wiki_docs():\n",
    "    documents = []\n",
    "    iterator_doc = 1\n",
    "    iterator_text = 1\n",
    "    for doc in os.listdir(\"./wiki-pages\"):\n",
    "        print(\"loading \",iterator_doc, \"page/109 pages\", end='\\r')\n",
    "        with json_lines.open('./wiki-pages/' + doc) as f:\n",
    "            for item in f:\n",
    "                documents.append(item)\n",
    "                iterator_text +=1\n",
    "        iterator_doc += 1\n",
    "    print(\"successfully loaded wiki pages!\")\n",
    "    return documents\n",
    "\n",
    "def calculate_term_frequency_wiki_docs(documents):\n",
    "    term_frequency_wiki_documents = Counter()\n",
    "    iterator_doc = 0\n",
    "    for document in documents:\n",
    "        term_frequency_wiki_documents.update(document['text'].lower()\n",
    "            .translate(str.maketrans('','',string.punctuation)).split()) \n",
    "        if iterator_doc % 10000 == 0:\n",
    "            print(\"calculate total word counts: \"\n",
    "                  ,\"%.2f\" % (iterator_doc/5416537 * 100)\n",
    "                  , \"% completed\", end='\\r')\n",
    "        iterator_doc += 1\n",
    "    print(\"calculation of word counts in wiki documents completed!\")\n",
    "    return term_frequency_wiki_documents\n",
    "\n",
    "def verify_zip_law(term_frequency_wiki_documents):\n",
    "    word_counts_zip_law = list(term_frequency_wiki_documents.values())\n",
    "    word_counts_zip_law.sort(reverse = True)\n",
    "    return word_counts_zip_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(term_frequency_wiki_documents):\n",
    "    return list(dict(term_frequency_wiki_documents).keys())\n",
    "\n",
    "def calculate_doc_frequency(documents, word_list):\n",
    "    df_dict = dict.fromkeys(word_list,0)\n",
    "    iterator_doc = 0\n",
    "    for document in documents:\n",
    "        words_in_doc = document['text'].lower().translate(str.maketrans('','',string.punctuation)).split()\n",
    "        if iterator_doc % 10000 == 0:\n",
    "            print(\"calculate document frequency: \"\n",
    "                  , \"%.2f\" % (iterator_doc/5416537 * 100)\n",
    "                  ,\"% completed\", end='\\r')\n",
    "        iterator_doc += 1\n",
    "        for word in set(words_in_doc):\n",
    "            df_dict[word] += 1\n",
    "    print(\"calculation of document frequency in wiki documents completed!\")    \n",
    "    return df_dict\n",
    "\n",
    "def calculate_term_frequency(text_string, word_list):\n",
    "    tf_dict = dict.fromkeys(word_list,0)\n",
    "    words_in_text = text_string.lower().translate(str.maketrans('','',string.punctuation)).split()\n",
    "    for word in words_in_text:\n",
    "        tf_dict[word] += 1\n",
    "    return tf_dict\n",
    "\n",
    "def load_training_datasets():\n",
    "    claim_id_list = [75397, 150448, 214861, 156709, 129629, 33078, 6744, 226034, 40190, 76253]\n",
    "    train_raw_dataset = []\n",
    "    claims_for_q2 = []\n",
    "    with open('./train.jsonl') as f:\n",
    "        print(\"loading training dataset\")\n",
    "        for train in json_lines.reader(f):\n",
    "            train_raw_dataset.append(train)\n",
    "            if train['id'] in claim_id_list:\n",
    "                claims_for_q2.append(train)\n",
    "    return train_raw_dataset, claims_for_q2\n",
    "\n",
    "def calculate_tf_wiki_documents(documents):\n",
    "    tf_wiki_docs = []\n",
    "    iterator_doc = 0\n",
    "    for document in documents:\n",
    "        tf_wiki_doc = dict()\n",
    "        tf_wiki_doc['id'] = document['id']\n",
    "        term_frequency_wiki_document = Counter()\n",
    "        term_frequency_wiki_document.update(document['text'].lower()\n",
    "            .translate(str.maketrans('','',string.punctuation)).split()) \n",
    "        tf_wiki_doc['tf'] = dict(term_frequency_wiki_document)\n",
    "        tf_wiki_docs.append(tf_wiki_doc)\n",
    "        if iterator_doc % 10000 == 0:\n",
    "            print(\"calculate term frequency for wiki docs: \"\n",
    "                  , \"%.2f\" % (iterator_doc/5416537 * 100)\n",
    "                  ,\"% completed\", end='\\r')\n",
    "        iterator_doc += 1\n",
    "    print(\"calculation of term frequency in wiki documents completed!\")    \n",
    "    return tf_wiki_docs\n",
    "\n",
    "def calculate_tf_claims(claims):\n",
    "    tf_claims = []\n",
    "    iterator_doc = 0\n",
    "    for claim in claims:\n",
    "        tf_claim = dict()\n",
    "        tf_claim['id'] = claim['id']\n",
    "        term_frequency_claim = Counter()\n",
    "        term_frequency_claim.update(claim['claim'].lower()\n",
    "            .translate(str.maketrans('','',string.punctuation)).split())\n",
    "        tf_claim['tf'] = dict(term_frequency_claim)\n",
    "        tf_claims.append(tf_claim)\n",
    "        print(\"calculate term frequency for claims: \"\n",
    "                  , \"%.2f\" % (iterator_doc)\n",
    "                  ,\" completed\", end='\\r')\n",
    "        iterator_doc += 1\n",
    "    print(\"calculation of term frequency in claims completed!\")    \n",
    "    return tf_claims\n",
    "\n",
    "def calculate_tfidf_wiki_docs(tf_wiki_docs,df_dict):\n",
    "    tfidf_wiki_docs = []\n",
    "    iterator_doc = 0\n",
    "    for tf_wiki_doc in tf_wiki_docs:\n",
    "        tfidf_wiki_doc = dict()\n",
    "        tfidf_wiki_doc['id'] = tf_wiki_doc['id']\n",
    "        tfidf_wiki_doc['tf'] = dict()\n",
    "        for word in tf_wiki_doc['tf'].keys():\n",
    "            tfidf_wiki_doc['tf'][word] = tf_wiki_doc['tf'][word]/df_dict[word]\n",
    "        tfidf_wiki_docs.append(tfidf_wiki_doc)\n",
    "        if iterator_doc % 10000 == 0:\n",
    "            print(\"calculate tf-idf for wiki docs: \"\n",
    "                  , \"%.2f\" % (iterator_doc/5416537 * 100)\n",
    "                  ,\"% completed\", end='\\r')\n",
    "        iterator_doc += 1\n",
    "    print(\"calculation of tfidf in wiki documents completed!\")\n",
    "    return tfidf_wiki_docs\n",
    "\n",
    "def calculate_tfidf_claims(tf_train_claims,df_dict):\n",
    "    tfidf_train_claims = []\n",
    "    iterator_doc = 0\n",
    "    for tf_train_claim in tf_train_claims:\n",
    "        tfidf_train_claim = dict()\n",
    "        tfidf_train_claim['id'] = tf_train_claim['id']\n",
    "        tfidf_train_claim['tf'] = dict()\n",
    "        for word in tf_train_claim['tf'].keys():\n",
    "            tfidf_train_claim['tf'][word] = tf_train_claim['tf'][word]/df_dict[word]\n",
    "        tfidf_train_claims.append(tfidf_train_claim)\n",
    "        \n",
    "        print(\"calculate tf-idf for training claims: \"\n",
    "                  , \"%.2f\" % (iterator_doc)\n",
    "                  ,\" completed\", end='\\r')\n",
    "        iterator_doc += 1\n",
    "    print(\"calculation of tfidf in claims completed!\")\n",
    "    return tfidf_train_claims\n",
    "\n",
    "def calculate_cosine_similarity(dict1,dict2):\n",
    "    numerator = 0\n",
    "    for word_in_both in dict1.keys() & dict2.keys():\n",
    "        numerator += dict1[word_in_both] * dict2[word_in_both]\n",
    "    dict1_vector = np.array(list(dict1.values()))\n",
    "    dict2_vector =  np.array(list(dict2.values()))\n",
    "    denominator = np.linalg.norm(dict1_vector) * np.linalg.norm(dict2_vector)\n",
    "    return numerator / denominator\n",
    "\n",
    "def calculate_top5(tfidf_train_claims, tfidf_wiki_docs):\n",
    "    top5_for_claims = dict()\n",
    "    claim_iterator = 0\n",
    "    for tfidf_train_claim in tfidf_train_claims:\n",
    "        print(\"calculating claim: \",claim_iterator)\n",
    "        claim_iterator += 1\n",
    "        \n",
    "        top5_for_claim = dict()\n",
    "        for tfidf_wiki_doc in tfidf_wiki_docs[0:5]:\n",
    "            iterator_doc = 0\n",
    "            top5_for_claim[tfidf_wiki_doc['id']] = calculate_cosine_similarity(tfidf_train_claim['tf'],tfidf_wiki_doc['tf'])\n",
    "        min_value_key = min(top5_for_claim, key=top5_for_claim.get)\n",
    "        min_value = top5_for_claim[min_value_key]\n",
    "        for tfidf_wiki_doc in tfidf_wiki_docs[5:]:\n",
    "            cos_sim = calculate_cosine_similarity(tfidf_train_claim['tf'],tfidf_wiki_doc['tf'])\n",
    "            if cos_sim > min_value:\n",
    "                del top5_for_claim[min_value_key]\n",
    "                top5_for_claim[tfidf_wiki_doc['id']] = cos_sim\n",
    "                min_value_key = min(top5_for_claim, key=top5_for_claim.get)\n",
    "                min_value = top5_for_claim[min_value_key]\n",
    "            if iterator_doc % 10000 == 0:\n",
    "                print(\"calculate cosine similarity between wiki docs and \", claim_iterator, \"th claim\"\n",
    "                  , \"%.2f\" % (iterator_doc/5416537 * 100)\n",
    "                  ,\"% completed\", end='\\r')\n",
    "            iterator_doc += 1\n",
    "        top5_for_claims[tfidf_train_claim['id']] = top5_for_claim\n",
    "    return top5_for_claims\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded wiki pages!\n",
      "calculation of word counts in wiki documents completed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b23922390>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#main() for q1\n",
    "documents = load_wiki_docs()   \n",
    "term_frequency_wiki_documents = calculate_term_frequency_wiki_docs(documents)    \n",
    "word_counts_zip_law = verify_zip_law(term_frequency_wiki_documents)\n",
    "plt.plot(word_counts_zip_law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculation of document frequency in wiki documents completed!\n",
      "loading training dataset\n"
     ]
    }
   ],
   "source": [
    "word_list = get_word_list(term_frequency_wiki_documents)\n",
    "df_dict = calculate_doc_frequency(documents,word_list)\n",
    "train_raw_dataset, claims_for_q2 = load_training_datasets()\n",
    "tf_train_claims = calculate_tf_claims(claims_for_q2, word_list)\n",
    "tf_wiki_docs = calculate_tf_wiki_documents(documents, word_list)\n",
    "del documents\n",
    "del term_frequency_wiki_documents\n",
    "del word_counts_zip_law\n",
    "del claims_for_q2\n",
    "del train_raw_dataset\n",
    "del word_list\n",
    "gc.collect()\n",
    "tfidf_train_claims = calculate_tfidf_claims(tf_train_claims,df_dict)\n",
    "tfidf_wiki_docs = calculate_tfidf_wiki_docs(tf_wiki_docs,df_dict)\n",
    "del tf_train_claims\n",
    "del tf_wiki_docs\n",
    "gc.collect()\n",
    "top5_docs_for_claims = calculate_top5(tfidf_train_claims, tfidf_wiki_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(word, document_unique_word_count, document_word_count, document, document_words):\n",
    "    if word not in document_words:\n",
    "        document[word] = 0\n",
    "    return (document[word] + 1)/(document_unique_word_count + document_word_count)\n",
    "\n",
    "\n",
    "def calculate_query_likelihood(query, document, smoothing_type):\n",
    "    query_likelihood = 1\n",
    "    if smoothing_type == \"laplace\":\n",
    "        document_words = document.keys()\n",
    "        document_unique_word_count = len(document_words)\n",
    "        document_word_count = sum(document.values())\n",
    "        for word in query:\n",
    "            query_likelihood *= laplace_smoothing(word, document_unique_word_count, document_word_count\n",
    "                                                  , document, document_words)\n",
    "#     elif smoothing_type == \"jm\":\n",
    "#         break\n",
    "#     elif smoothing_type == \"dirichlet\":\n",
    "#         break\n",
    "    return query_likelihood\n",
    "\n",
    "def calculate_top5_prob(claims_for_q2, tf_wiki_docs, smoothing_type):  \n",
    "    top5_for_claims_prob = dict()\n",
    "    claim_iterator = 0\n",
    "    for claim in claims_for_q2:\n",
    "        claim_iterator += 1\n",
    "        query = claim['claim'].lower().translate(str.maketrans('','',string.punctuation)).split()\n",
    "        top5_for_claim_prob = dict()\n",
    "        for tf_wiki_doc in tf_wiki_docs[0:5]:\n",
    "            iterator_doc = 0\n",
    "            document = tf_wiki_doc['tf']\n",
    "            top5_for_claim_prob[tf_wiki_doc['id']] = calculate_query_likelihood(query, document, smoothing_type)\n",
    "        min_value_key = min(top5_for_claim_prob, key=top5_for_claim_prob.get)\n",
    "        min_value = top5_for_claim_prob[min_value_key]\n",
    "        max_vk = max(top5_for_claim_prob, key=top5_for_claim_prob.get)\n",
    "        max_v = top5_for_claim_prob[max_vk]\n",
    "        print(\"startmax\", claim['id'], max_v)\n",
    "        print(\"startmin\", claim['id'], min_value)\n",
    "        for tf_wiki_doc in tf_wiki_docs[5:]:\n",
    "            document = tf_wiki_doc['tf']\n",
    "            query_likelihood = calculate_query_likelihood(query, document, smoothing_type)\n",
    "            if query_likelihood > min_value:\n",
    "                del top5_for_claim_prob[min_value_key]\n",
    "                top5_for_claim_prob[tf_wiki_doc['id']] = query_likelihood\n",
    "                min_value_key = min(top5_for_claim_prob, key=top5_for_claim_prob.get)\n",
    "                min_value = top5_for_claim_prob[min_value_key]\n",
    "                max_vk = max(top5_for_claim_prob, key=top5_for_claim_prob.get)\n",
    "                max_v = top5_for_claim_prob[max_vk]\n",
    "            if iterator_doc % 10000 == 0:\n",
    "                print(\"calculate query likelihood between wiki docs and \", claim_iterator, \"th claim\"\n",
    "                  , \"%.2f\" % (iterator_doc/5416537 * 100)\n",
    "                  ,\"% completed\", end='\\r')\n",
    "            iterator_doc += 1\n",
    "        top5_for_claims_prob[claim['id']] = top5_for_claim_prob\n",
    "    return top5_for_claims_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded wiki pages!\n",
      "calculation of word counts in wiki documents completed!\n",
      "loading training dataset\n",
      "calculation of term frequency in wiki documents completed!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calculate_top5_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bc1a04b14fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_raw_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclaims_for_q2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_training_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtf_wiki_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_tf_wiki_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtop5_docs_for_claims_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_top5_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaims_for_q2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_wiki_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"laplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_top5_prob' is not defined"
     ]
    }
   ],
   "source": [
    "documents = load_wiki_docs()\n",
    "train_raw_dataset, claims_for_q2 = load_training_datasets()\n",
    "tf_wiki_docs = calculate_tf_wiki_documents(documents)\n",
    "top5_docs_for_claims_prob = calculate_top5_prob(claims_for_q2, tf_wiki_docs, \"laplace\")\n",
    "# to do: other 2 smoothing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
